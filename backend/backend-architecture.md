# AI Content Detector - Backend Architecture

## Overview

The AI Content Detector Chrome extension requires a robust, scalable backend service to analyze web content and determine whether it was generated by AI or written by humans. This architecture document outlines the design of a backend system built on Cloudflare Workers and Supabase, chosen for their optimal performance, cost efficiency, and global distribution capabilities.

The system is designed to handle millions of content analysis requests while maintaining sub-100ms response times and operating within free tier constraints. The architecture prioritizes intelligent caching, cost optimization, and user privacy while providing a seamless experience for Chrome extension users.

## Technology Stack

### Core Infrastructure

**Cloudflare Workers** serves as our primary API layer, providing serverless execution at the edge. This choice offers several critical advantages:

- **Global Edge Network**: With 200+ locations worldwide, requests are processed at the nearest edge location, resulting in 10-50ms latency regardless of user location
- **Zero Cold Starts**: Unlike traditional serverless platforms, Cloudflare Workers maintain warm instances, ensuring consistent sub-100ms response times
- **V8 Engine Performance**: The JavaScript/TypeScript runtime is optimized for API processing, often outperforming Python-based alternatives
- **Isolated Execution**: Each request runs in complete isolation, preventing resource contention and ensuring predictable performance

**Supabase** provides our database layer with PostgreSQL, offering enterprise-grade features within a generous free tier:

- **500MB Database**: Sufficient for caching millions of analysis results
- **Built-in Authentication**: Leveraging Supabase Auth for user management
- **Real-time Capabilities**: Supporting live updates and subscriptions
- **Row Level Security**: Ensuring data privacy and access control

### Performance Benefits

The combination of Cloudflare Workers and Supabase creates a highly performant system:

- **Global Distribution**: Content analysis happens at the edge, closest to users worldwide
- **Instant Scalability**: Auto-scaling based on demand without manual intervention
- **Consistent Performance**: No cold starts or performance degradation during traffic spikes
- **Cost Efficiency**: Pay-per-request model with generous free tiers

### Free Tier Constraints

Our architecture is designed to operate within free tier limits while supporting substantial user growth:

- **Cloudflare Workers**: 100,000 requests per day
- **Supabase**: 500MB database storage, 50,000 monthly active users
- **No Performance Penalties**: Free tier maintains full performance characteristics
- **Global Edge Network**: All 200+ locations available in free tier

## API Design

### Quick Analysis Endpoint

The primary API endpoint handles content analysis requests from the Chrome extension:

**Endpoint**: `POST https://api.aidetector.com/v1/analyze-quick`

**Request Structure**:
The API accepts a JSON payload containing:
- **URL**: The webpage being analyzed (used for caching and tracking)
- **Text**: The first 500 words of content for quick analysis
- **User ID**: Anonymous identifier for usage tracking and rate limiting
- **Mode**: Analysis mode (quick or deep)
- **Last Modified**: Page modification timestamp for cache invalidation

**Response Structure**:
The API returns a structured response including:
- **Classification**: Binary result (AI-generated or human-written)
- **Confidence Level**: High, medium, or low confidence in the classification
- **Confidence Score**: Numerical score (0.0 to 1.0) for detailed analysis
- **Key Indicators**: Specific patterns that influenced the classification
- **Reasoning**: Human-readable explanation of the decision
- **Metadata**: Analysis details including word count, provider used, and cache status

### API Design Principles

The API is designed with several key principles:

- **Stateless Operation**: Each request is independent, enabling horizontal scaling
- **Idempotent Responses**: Identical requests return identical results
- **Graceful Degradation**: System continues operating even during partial failures
- **Backward Compatibility**: API versioning ensures extension compatibility

## Database Schema

### URL Cache Table

The cache table stores analysis results to minimize API costs and improve response times:

**Table Structure**:
- **Primary Key**: UUID for unique identification
- **URL Hash**: SHA-256 hash of normalized URL for efficient lookups
- **Content Hash**: SHA-256 hash of analyzed content for change detection
- **Last Modified**: Page modification timestamp for cache invalidation
- **Full URL**: Complete URL for reference and debugging
- **Domain**: Extracted domain for analytics and filtering
- **Mode**: Analysis mode (quick or deep) affecting cache strategy
- **Result**: JSONB field containing complete analysis results
- **Created At**: Timestamp when the cache entry was created for analytics and debugging

**Indexing Strategy**:
The table uses a composite unique index on (url_hash, content_hash, last_modified, mode) to ensure cache uniqueness while enabling fast lookups. This design allows the system to detect when content has changed and invalidate stale cache entries automatically.

### Usage Analytics Table

A simplified analytics table tracks user activity for abuse prevention and service optimization:

**Table Structure**:
- **User ID**: Anonymous identifier for tracking individual usage
- **Daily Count**: Number of analyses performed per day per user
- **Created At**: Timestamp for temporal analysis

This simplified design addresses the comment about making analytics "very simple" - instead of tracking complex metrics, we focus on daily usage counts per user for rate limiting and abuse prevention.

## Caching Strategy

### Cache Key Generation

The caching system uses a sophisticated multi-factor key generation strategy:

**Primary Components**:
- **URL Normalization**: Domain and path extraction for consistent hashing
- **Content Hashing**: SHA-256 of analyzed text for change detection
- **Temporal Tracking**: Last-modified timestamp for content freshness
- **Mode Differentiation**: Separate caches for quick and deep analysis modes

**Cache Key Format**: `{url_hash}:{content_hash}:{last_modified}:{mode}`

This approach ensures that identical content is cached once regardless of how many users analyze it, while automatically detecting when content has changed and requiring fresh analysis.

### Handling Missing Last-Modified Timestamps

The system gracefully handles scenarios where `last_modified` timestamps are unavailable:

**Common Scenarios**:
- **Static sites** without proper HTTP headers
- **JavaScript-heavy SPAs** that don't set last-modified headers
- **CDN-cached content** with generic timestamps
- **API responses** that don't include modification dates
- **User-generated content** without reliable timestamps

**Smart Fallback Strategy**:
```javascript
const lastModified = request.last_modified || 'unknown';
const cacheKey = `${url_hash}:${content_hash}:${last_modified}:${mode}`;

// Cache lookup logic
if (cachedResult) {
  if (request.last_modified && cachedResult.last_modified >= request.last_modified) {
    return cachedResult; // Use cache - content hasn't changed
  } else if (!request.last_modified && cachedResult.last_modified === 'unknown') {
    return cachedResult; // Use cache - both unknown timestamps
  } else {
    // Make new API call - either timestamp changed or we now have timestamp
    return await analyzeContent(text);
  }
}
```

**Benefits**:
- **Preserves full cache key format** when timestamps are available
- **Gracefully handles missing timestamps** without breaking the system
- **Balances cache efficiency** with content freshness
- **Maintains backward compatibility** with existing cached entries
- **Two-layer change detection**: Content hash for analyzed text changes, timestamp for page-level modifications

### Cache Lookup Process

The cache lookup follows a systematic approach:

1. **Request Reception**: Extension sends analysis request with URL, text, and metadata
2. **Key Generation**: Backend creates cache key from URL, content hash, and timestamps
3. **Cache Query**: Database lookup using composite key with expiration check
4. **Result Handling**: If found, return cached result immediately; if not, proceed to AI analysis
5. **Cache Storage**: Store new analysis results with appropriate TTL

### Cache TTL Strategy

Different content types require different caching strategies:

- **Quick Mode**: 7-day TTL for static content that rarely changes
- **Deep Mode**: 3-day TTL for detailed analysis that may become stale
- **Dynamic Content**: 1-2 day TTL for news sites and frequently updated content
- **Popular URLs**: Extended TTL based on access frequency and content stability

### Cache Invalidation

The system employs multiple invalidation strategies:

- **Content Change Detection**: Hash comparison identifies modified content
- **Manual Invalidation**: Administrative tools for clearing specific domains
- **Bulk Cleanup**: Periodic cleanup of old entries to maintain performance

### Storage Optimization

The caching system is optimized for the 500MB free tier constraint:

- **Compression**: Gzip compression reduces JSON storage by 60-70%
- **Efficient Schema**: Minimal metadata storage with JSONB for flexible results
- **Manual Cleanup**: Periodic removal of old entries to manage storage
- **Estimated Capacity**: 50,000+ cached results within free tier limits

## Rate Limiting and Usage Management

### Free Quick Mode

The system provides a generous free tier with intelligent abuse prevention:

- **Daily Limits**: 50 analyses per day per user, sufficient for normal usage
- **Rate Limiting**: 1 request per 5 seconds to prevent API abuse
- **Simple Tracking**: Daily count per user for straightforward enforcement
- **Clear Messaging**: User-friendly error messages when limits are exceeded

### API Key Mode

For power users and developers, the system supports direct API key usage:

- **No Backend Limits**: Users with their own API keys bypass our rate limits
- **Provider Limits**: Subject to OpenAI/Claude's rate limits and quotas
- **Cost Responsibility**: Users control their own API costs and usage
- **Optional Tracking**: Display cost estimates for user awareness

### Abuse Prevention

The system employs multiple layers of abuse prevention:

- **User Identification**: Anonymous but persistent user IDs for tracking
- **Pattern Detection**: Identify unusual usage patterns and automated requests
- **Progressive Limits**: Gradual restrictions for suspected abuse
- **Appeal Process**: Manual review for legitimate high-volume users

## Error Handling and Resilience

### API Failure Scenarios

The system gracefully handles various failure modes:

- **AI Provider Outages**: Automatic fallback between OpenAI and Claude
- **Network Issues**: Retry logic with exponential backoff
- **Database Failures**: Continue operation with degraded caching
- **Rate Limit Exceeded**: Clear error messages with retry guidance

### Graceful Degradation

When components fail, the system maintains core functionality:

- **Cache-Only Mode**: Serve cached results when AI providers are unavailable
- **Local Heuristics**: Basic analysis using built-in detection algorithms
- **User Notification**: Clear communication about service status
- **Recovery Procedures**: Automatic restoration when services return

### Error Communication

Users receive helpful error messages that explain:

- **What Went Wrong**: Clear description of the issue
- **What Users Can Do**: Specific actions to resolve the problem
- **When to Retry**: Guidance on timing for retry attempts
- **Alternative Options**: Suggestions for workarounds

## Security and Privacy

### Data Privacy

The system is designed with privacy as a core principle:

- **No Content Storage**: Analyzed text is never stored in the database
- **Hash-Based Caching**: Only cryptographic hashes are stored for cache keys
- **Anonymous Tracking**: User IDs are anonymous and not tied to personal data
- **Optional Analysis**: Users can disable analysis on sensitive sites

### API Key Security

For users with their own API keys:

- **Secure Storage**: Keys stored in Chrome's encrypted sync storage
- **Local Validation**: Keys validated before transmission
- **No Logging**: API keys never logged or stored on our servers
- **User Control**: Complete user responsibility for key management

### Infrastructure Security

The platform leverages enterprise-grade security:

- **HTTPS Everywhere**: All communications encrypted in transit
- **Row Level Security**: Database-level access controls
- **Environment Variables**: Sensitive configuration isolated from code
- **Regular Audits**: Security reviews and vulnerability assessments

## Deployment Architecture

### Cloudflare Workers Deployment

The API layer is deployed using Cloudflare's serverless platform:

- **Git Integration**: Automatic deployment from version control
- **Environment Management**: Secure handling of API keys and configuration
- **Custom Domain**: Professional `api.aidetector.com` endpoint
- **SSL Certificate**: Automatic HTTPS with modern TLS standards

### Supabase Database Setup

The database layer is configured for production use:

- **PostgreSQL Instance**: Enterprise-grade database with full SQL support
- **Connection Pooling**: Optimized database connections for high concurrency
- **Backup Strategy**: Automated backups with point-in-time recovery
- **Monitoring**: Real-time performance and usage monitoring

### Monitoring and Observability

The system includes comprehensive monitoring:

- **Request Volume**: Track usage patterns and growth trends
- **Cache Effectiveness**: Monitor hit rates and storage utilization
- **Error Tracking**: Identify and resolve issues quickly
- **Cost Monitoring**: Track API usage and optimize spending

## Cost Management

### Smart Caching Strategy

Aggressive caching minimizes API costs:

- **Hit Rate Optimization**: Target 80%+ cache hit rate to reduce API calls
- **Provider Routing**: Intelligent selection between OpenAI and Claude based on cost and availability
- **Content Prioritization**: Cache popular content with longer TTL
- **Storage Efficiency**: Compress and optimize cached data

### Free Tier Optimization

The system is designed to maximize free tier value:

- **Efficient Storage**: Optimize database schema for minimal storage usage
- **Smart TTL**: Balance cache freshness with storage costs
- **Cleanup Automation**: Remove expired entries to maintain performance
- **Usage Monitoring**: Track resource consumption to stay within limits

### Scaling Considerations

As the service grows, cost management becomes more critical:

- **Volume Discounts**: Negotiate better rates with AI providers
- **Multi-Provider Strategy**: Distribute load across multiple AI services
- **Regional Optimization**: Route requests to cost-effective regions
- **Predictive Caching**: Pre-cache popular content during off-peak hours

## Future Enhancements

### Advanced Features

The architecture supports several planned enhancements:

- **Batch Processing**: Analyze multiple URLs in a single request
- **Webhook Integration**: Real-time notifications for analysis completion
- **API Versioning**: Maintain backward compatibility during updates
- **Advanced Analytics**: Detailed insights into usage patterns and trends

### Scaling Architecture

The system is designed to scale with user growth:

- **Database Sharding**: Distribute data across multiple database instances
- **CDN Integration**: Leverage Cloudflare's global CDN for static assets
- **Microservices**: Split functionality into specialized services
- **Kubernetes Deployment**: Container orchestration for complex deployments

### Performance Optimizations

Future improvements will focus on performance:

- **Edge Computing**: Move more processing to edge locations
- **Predictive Loading**: Pre-cache content based on user behavior
- **Compression Optimization**: Advanced compression for faster transfers
- **Connection Pooling**: Optimize database connections for high concurrency

## Conclusion

This architecture provides a robust, scalable foundation for the AI Content Detector Chrome extension. By leveraging Cloudflare Workers and Supabase, the system achieves enterprise-grade performance while operating within free tier constraints. The intelligent caching strategy, comprehensive error handling, and privacy-focused design ensure a reliable and secure service for users worldwide.

The modular design allows for future enhancements while maintaining backward compatibility and performance standards. The cost-optimized approach ensures sustainable operation as the user base grows, while the security and privacy measures protect user data and maintain trust. 